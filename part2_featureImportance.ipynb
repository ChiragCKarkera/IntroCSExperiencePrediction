{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Factors that Predict Intro CS Experience Based on Gender: Part Two\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "import IPython.core.display as di # Example: di.display_html('<h3>%s:</h3>' % str, raw=True)\n",
    "\n",
    "# This line will hide code by default when the notebook is exported as HTML\n",
    "di.display_html('<script>jQuery(function() {if (jQuery(\"body.notebook_app\").length == 0) { jQuery(\".input_area\").toggle(); jQuery(\".prompt\").toggle();}});</script>', raw=True)\n",
    "\n",
    "# This line will add a button to toggle visibility of code blocks, for use with the HTML export version\n",
    "di.display_html('''<button onclick=\"jQuery('.input_area').toggle(); jQuery('.prompt').toggle();\">Toggle code</button>''', raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "I am interested in identify the leading indicators of experience broken down by gender in introductory CS at an elite research university like Berkeley. In short, I want to find the attributes that split the dataset as *purely* as possible into male and female.\n",
    "\n",
    "To solve this problem, I will undertake the following course of action:\n",
    "1. Explore the dataset\n",
    "    - Usually, I would explore the dataset to ensure its integrity and understand the context. But in this case, I will skip this step since I designed the study and collected the data, as such, I am well versed of the context. Further, I have done previous work on this dataset, so I know its boundaries.\n",
    "2. Identify features that may be used. \n",
    "    - If possible, engineer features that might provide greater discrimination.\n",
    "3. With the understanding that this a **classification** task, explore a couple of classifiers that might be well suited for the problem at hand.\n",
    "    - Random Forest classifier\n",
    "    - eXtreme Gradient Boosted (XGBoost) trees classifier\n",
    "    - Support Vector Machine (SVM)\n",
    "    - Decision Tree classifier\n",
    "4.  Select appropriate classifier based on evaluation metric and tune it for optimality.\n",
    "5.  Extract top features responsible for discriminating the data.\n",
    "\n",
    "I have already completed steps one and two in this [notebook](https://github.com/omoju/genderCSExperience/blob/master/genderedCSExperience.ipynb). Now I will focus on steps three through five.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from __future__ import division\n",
    "import sys\n",
    "sys.path.append('tools/')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tools\n",
    "\n",
    "   \n",
    "# Graphing Libraries\n",
    "import matplotlib.pyplot as pyplt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_pickle('data/features.pickle.dat')\n",
    "y = pd.read_pickle('data/labels.pickle.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, decide how many training vs test samples you want\n",
    "num_all = X.shape[0]  # same as len(student_data)\n",
    "num_train = 662  # about 75% of the data\n",
    "num_test = num_all - num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "def shuffle_split_data(X, y):\n",
    "    \"\"\" Shuffles and splits data into 75% training and 25% testing subsets,\n",
    "        then returns the training and testing subsets. \"\"\"\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, \n",
    "                                                                        train_size=num_train, random_state=42)\n",
    "    \n",
    "    # Return the training and testing data subsets\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully shuffled and split the data!\n",
      "Training set: 662 samples\n",
      "Test set: 220 samples\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "try:\n",
    "    X_train, y_train, X_test, y_test = shuffle_split_data(X, y)\n",
    "    print \"Successfully shuffled and split the data!\"\n",
    "except:\n",
    "    print \"Something went wrong with shuffling and splitting the data.\"\n",
    "\n",
    "\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling and Validating\n",
    "\n",
    "### Algorithms and Techniques\n",
    "\n",
    "For the problem of determining the factors that predict intro CS experience based on gender, I experimented with four different classifiers, a decision tree classifier, two ensemble methods and a support vector machine:\n",
    "\n",
    "- I selected a **Random Forest classifier** because it is considered one of the best off-the-shelf learning algorithm, and requires almost no tuning. \n",
    "\n",
    "- I selected an **eXtreme Gradient Boosted (XGBoost) trees classifier**; which is an advanced implementation of the gradient boosting algorithm. From reading literature on machine learning in practice, the XGBoost classifier has differentiated itself as a classifier that has successfully demonstrated its performance in a wide range of problems. For example, \"among the 29 challenge winning solutions published at Kaggle's blog during 2015, 17 solutions used XGBoost.\"\n",
    "\n",
    "- I selected a **Support Vector Machine (SVMs)** because they are very robust classifiers and *more importantly*, they have a method to correct for class imbalances. \n",
    "              \n",
    "- Finally I selected a **Decision Tree classifier** because it lends itself to interpretability. For this problem domain, it is not just satisfactory for me to discriminate between male and female students, what I ultimately want is to gain *insights* into what the salient factors around the experience of intro CS are, based on gender.\n",
    "\n",
    "I implemented the four learning algorithms. For each of the learners I implemented the baseline algorithm using a stratified shuffle split cross validation with 10 folds and calculated the F1 scores and looked at the confusion matrices respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X_train # Training data\n",
    "seed = 342 # For reproducability\n",
    "folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "models = {\n",
    "          'XGBoost': XGBClassifier(),\n",
    "          'DecisionTree': DecisionTreeClassifier(),\n",
    "          'SVC': svm.SVC(),\n",
    "          'RandomForest': RandomForestClassifier()\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION RESULTS OF BASELINE CLASSIFIERS\n",
      "\n",
      "CLASSIFIER           MEAN SCORE %  STD DEV % \n",
      "RandomForest             54.93        5.37   \n",
      "DecisionTree             51.08       10.93   \n",
      "XGBoost                  59.69        6.02   \n",
      "SVC                      55.39        6.67   \n"
     ]
    }
   ],
   "source": [
    "print \"CLASSIFICATION RESULTS OF BASELINE CLASSIFIERS\\n\"\n",
    "print\"{:20}{:^15}{:^10}\".format('CLASSIFIER', 'MEAN SCORE %', 'STD DEV %')\n",
    "\n",
    "\n",
    "for model_name, model in models.iteritems():\n",
    "    kfold = StratifiedKFold(y_train, n_folds=folds, random_state=np.random.seed(seed))\n",
    "    results = cross_val_score(model, X, y_train, cv=kfold, scoring='f1')\n",
    "    print\"{:20}{:^15.2f}{:^10.2f}\".format(model_name, results.mean()*100, results.std()*100)\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Features_test = X_test\n",
    "pred_results = {}\n",
    "\n",
    "for model_name, model in models.iteritems():\n",
    "    # make predictions for test data\n",
    "    model.fit(X, y_train)\n",
    "    y_predictions = model.predict(Features_test)\n",
    "    predictions = [round(value) for value in y_predictions]\n",
    "    \n",
    "    # evaluate predictions\n",
    "    C = confusion_matrix(y_test, predictions)\n",
    "    pred_results[model_name] = dict(score=metrics.f1_score(y_test, predictions) * 100,\n",
    "                              C=C, expected_value=0, rates=[[0,0],[0,0]]) \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Expected Value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate expected rates\n",
    "\n",
    "for key in pred_results:\n",
    "    rates = tools.expected_rates(pred_results[key]['C'])\n",
    "    pred_results[key]['rates'] = [[1-rates['fpr'], rates['fpr']],[1-rates['tpr'], rates['tpr']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost_benefit_matrix = [[1, -1],[-2, 5]]\n",
    "\n",
    "# calculate expected value\n",
    "for key in pred_results:\n",
    "    pred_results[key]['expected_value'] = 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            pred_results[key]['expected_value'] += pred_results[key]['rates'][i][j] * cost_benefit_matrix[i][j]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PREDICTION RESULTS OF BASELINE CLASSIFIERS\n",
      "    \n",
      "CLASSIFIER              SCORE %    EXPECTED VALUE \n",
      "Majority                 58.01          0.00      \n",
      "RandomForest             55.91          1.87      \n",
      "XGBoost                  70.35          3.11      \n",
      "DecisionTree             62.44          2.63      \n",
      "SVC                      53.57          1.63      \n"
     ]
    }
   ],
   "source": [
    "print \"{:^50}\".format(\"PREDICTION RESULTS OF BASELINE CLASSIFIERS\\n\")\n",
    "print\"{:20}{:^15}{:^15}\".format('CLASSIFIER', 'SCORE %', 'EXPECTED VALUE')\n",
    "print\"{:20}{:^15.2f}{:^15.2f}\".format('Majority', y_train.tolist().count(0) / len(y_train) * 100, 0)\n",
    "for key in pred_results:\n",
    "    print\"{:20}{:^15.2f}{:^15.2f}\".format(key, pred_results[key]['score'], pred_results[key]['expected_value'])\n",
    "    \n",
    "\n",
    "sns.set(font_scale = 1)    \n",
    "for key in pred_results:    \n",
    "    tools.show_confusion_matrix(pred_results[key]['C'], \n",
    "                                key,'report/figures/'+key+'.png', ['Class Male', 'Class Female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select Best Classifier\n",
    "\n",
    "### Benchmark\n",
    "Before I start selecting which classifier I want to proceed with, I need a **baseline** score on which I can evaluate the practical value of datamining for this problem. Since this project is applying machine learning to a novel dataset, I do not have standard benchmarks I can measure against. As such, I have decided to use a simple *majority* classifier which always selects the majority class of the training set. \n",
    "\n",
    "\n",
    "|Confusion Matrices|\n",
    "|---|---|\n",
    "| <img src=\"report/figures/XGBoost.png\" alt=\"Drawing\" style=\"width: 350px;\"/>  | <img src=\"report/figures/SVC.png\" alt=\"Drawing\" style=\"width: 350px;\"/> |\n",
    "| <img src=\"report/figures/DecisionTree.png\" alt=\"Drawing\" style=\"width: 350px;\"/>  | <img src=\"report/figures/RandomForest.png\" alt=\"Drawing\" style=\"width: 350px;\"/> |\n",
    "\n",
    "#### Expected Value\n",
    "\n",
    "To evaluate the performance of the classifiers I consider two separate evaluation metrics, the **F_1** score of the classifiers and the **expected value**<a href=\"#fn1\" id=\"ref1\">1</a> result. The equation for the expected value is calculated as follows:\n",
    "$$ p(\\mathbf{p}).[p(\\mathbf{Y}|\\mathbf{p}). b(\\mathbf{Y},\\mathbf{p}) + p(\\mathbf{N}|\\mathbf{p}). b(\\mathbf{N},\\mathbf{p})] + p(\\mathbf{n}).[p(\\mathbf{N}|\\mathbf{n}). b(\\mathbf{N},\\mathbf{n}) + \n",
    "p(\\mathbf{Y}|\\mathbf{n}). b(\\mathbf{Y},\\mathbf{n})]$$\n",
    "In short, it is the expected rates multiplied by the cost-benefit of each entry in the confusion matrix, weighted by the class priors. I invented a cost and benefits value associated with entry of the confusion matrix based on domain knowledge. My goal is to reward correct female class classification while penalizing false classifications. My choices for these values can be see in table below.\n",
    "\n",
    "<img src=\"report/figures/expectedValueCalculation.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "|Cost and Benefits|Value|\n",
    "|---|---|\n",
    "|Benefit of *correctly identifying* a **female** student  | 5  |\n",
    "|Benefit of *correctly identifying* a **male** student  | 1  |\n",
    "|Cost of *misclassifying* a **female** student  | -2  |\n",
    "|Cost of *misclassifying* a **male** student  | 1  |\n",
    "\n",
    "\n",
    "I am using these two evaluation metrics in tandem because simple accuracy does not distinguish between **false positive** and **false negative** rates. Furthermore, the student dataset has unbalanced classes which will also cause the accuracy metric to breakdown as the classes become more skewed. \n",
    "\n",
    "From running these baseline classifiers, I selected the **xgboost classifier** as the best classifier based on my evaluation criteria. On this problem, Random Forest classifier and the Support Vector Machine did not give a better performance than the naive classifier. While the Decision Tree did well, it was not as robust as the XGBoost classifier.\n",
    "\n",
    "Its interesting to note that the Decision Tree classifier, had the highest true positive rate at 0.63, however, its false positive rate was a staggering 0.38! This means that it cannot find a meaningful sets of conditions for separating males from females in the dataset. The Support Vector Machine had the lowest false positive rate but did not beat the majority classifier because its true positive rate was abysmal. Where as, XGBoost does satisfactory on both fronts.\n",
    "\n",
    "To idenitfy factors that predict experience based on gender, I will then extract the top features in discriminating the data. I will expand the final step to:\n",
    "- Explore the various parameters around feature splitting \n",
    "    - Xgboost algorithm feature importance score\n",
    "    - Information gain\n",
    "    - Cover\n",
    "    \n",
    "    \n",
    " \n",
    "<sup id=\"fn1\">1. Image from \"Provost and Fawcett. Data Science for Business: What You Need to Know About Data Mining and Data-analytic Thinking, 2013\"</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'report/figures/SecondXGraph.png'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X, y_train)\n",
    "    \n",
    "    \n",
    "g = xgb.to_graphviz(model, num_trees=1, rankdir='TB')\n",
    "g.format = 'png'\n",
    "g.render('report/figures/firstXGraph', view=False)\n",
    "g = xgb.to_graphviz(model, num_trees=2, rankdir='TB')\n",
    "g.format = 'png'\n",
    "g.render('report/figures/SecondXGraph', view=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "importances = model.booster().get_fscore()\n",
    "importance_frame = pd.DataFrame({'Importance': list(importances.values()), 'Feature': list(importances.keys())})\n",
    "importance_frame.sort_values(by = 'Importance', inplace = True)\n",
    "importance_frame.plot(kind = 'barh', x = 'Feature', figsize = (8,14),  color = 'orange');\n",
    "pyplt.title('XGBOOST FEATURE IMPORTANCE')\n",
    "pyplt.savefig('report/figures/featureImportance.png', dpi=200, bbox_inches='tight')\n",
    "pyplt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tune Xgboost Model\n",
    "\n",
    "I am going to tune my model based on some heuristics about the kinds of value ranges that are suitable for the hyper-parameters I want to learn.\n",
    "\n",
    "#### Ideal choices of parameters as taken from Xgboost With Python ebook\n",
    "- Number of Trees (`n_estimators`) set to a fixed value between 100 and 1000, depending on the dataset size.\n",
    "- Learning Rate (`learnin_rate`) simplified to the ratio: [2 to 10]/trees, depending on the number of trees.\n",
    "- Row Sampling (`subsample`) grid searched values in the range [0.5, 0.75, 1.0].\n",
    "- Column Sampling (`colsample` bytree and maybe colsample bylevel) grid searched values in the range [0.4, 0.6, 0.8, 1.0].\n",
    "- Min Leaf Weight (`min_child_weight`) simplified to the ratio 3/rare_events , where rare events rare events is the percentage of rare event observations in the dataset.\n",
    "- Tree Size (`max_depth`) grid searched values in the rage [4, 6, 8, 10].\n",
    "- Min Split Gain (`gamma`) fixed with a value of zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "# Build a stratified shuffle object because of unbalanced data\n",
    "ssscv = StratifiedShuffleSplit(y_train, folds, random_state=np.random.seed(seed))\n",
    "\n",
    "num_trees = range(200, 1100, 100)\n",
    "params_grid = {\n",
    "            'learning_rate': [x/y for x in range(2, 3, 1) for y in num_trees],\n",
    "            'max_depth':  [4, 6, 8, 10, 12],\n",
    "            'n_estimators': num_trees,\n",
    "            'colsample_bytree': [0.4, 0.6, 0.8, 1.0],\n",
    "            'subsample':[0.7]\n",
    "}\n",
    "\n",
    "params_fixed = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hyper-parameters\n",
      "{'reg_alpha': 0, 'colsample_bytree': 1.0, 'silent': 1, 'colsample_bylevel': 1, 'scale_pos_weight': 1, 'learning_rate': 0.005, 'missing': nan, 'max_delta_step': 0, 'nthread': -1, 'base_score': 0.5, 'n_estimators': 900, 'subsample': 0.7, 'reg_lambda': 1, 'seed': 0, 'min_child_weight': 1, 'objective': 'binary:logistic', 'max_depth': 6, 'gamma': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load and use already tuned classifier, else tune classifier\n",
    "\n",
    "tune_flag = False\n",
    "model_filename = \"data/fixedDepthTree.pickle.dat\"\n",
    "\n",
    "if tune_flag:\n",
    "    grid = GridSearchCV(estimator=XGBClassifier(**params_fixed),\n",
    "        param_grid=params_grid,\n",
    "        cv=ssscv,\n",
    "        scoring='f1')\n",
    "    grid.fit(X, y_train)\n",
    "    \n",
    "    print \"Best accuracy obtained: {0}\".format(grid.best_score_)\n",
    "    print \"Parameters:\"\n",
    "    for key, value in grid.best_params_.items():\n",
    "        print \"\\t{}: {}\".format(key, value)\n",
    "    model = grid.best_estimator_\n",
    "    \n",
    "    # save model to file\n",
    "    pickle.dump(model, open(model_filename, \"wb\"))\n",
    "    print \"Model hyper-parameters\\n\", model.get_params()\n",
    "    \n",
    "else:    \n",
    "    model = tools.load_model(model_filename)\n",
    "    print \"Model hyper-parameters\\n\", model.get_params()\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of tuned classifier: 73.17%\n",
      "Tuned Model improvement over baseline classifier: 2.82%\n"
     ]
    }
   ],
   "source": [
    "Features_test = X_test\n",
    "\n",
    "# make predictions for test data\n",
    "model.fit(X, y_train)\n",
    "ypred = model.predict(Features_test)\n",
    "predictions = ypred\n",
    "\n",
    "# evaluate predictions\n",
    "xgb_tuned_prediction_score = f1_score(y_test, predictions) * 100\n",
    "\n",
    "print \"Prediction of tuned classifier: %.2f%%\"%(xgb_tuned_prediction_score)\n",
    "print \"Tuned Model improvement over baseline classifier: %.2f%%\"%(xgb_tuned_prediction_score - \n",
    "                                                                  pred_results['XGBoost']['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1)\n",
    "model_name = model.__class__.__name__\n",
    "\n",
    "C = confusion_matrix(y_test, ypred)\n",
    "tools.show_confusion_matrix(C, model_name, 'report/figures/tuned_model_CM.png', ['Class Male', 'Class Female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'report/figures/second_tuned_model_graph.png'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y_train)\n",
    "g = xgb.to_graphviz(model, num_trees=1, rankdir='TB')\n",
    "g.format = 'png'\n",
    "g.render('report/figures/first_tuned_model_graph', view=False)\n",
    "\n",
    "g = xgb.to_graphviz(model, num_trees=2, rankdir='TB')\n",
    "g.format = 'png'\n",
    "g.render('report/figures/second_tuned_model_graph', view=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of tuning\n",
    "Once I performed the search through the hyper-parameter space to find the combination of hyper-parameters that maximized the performance of the selected classifier, I was able to **improve** the previous `F_1` score **by 2.82%**, to achieve a prediction score of 73.17%.\n",
    "- You can see that the **false negative** count for the female class has gone from 40 down to 35. This decision cost us a very small increase in the **false positive** count of the male class from 19 to 20. This is not too bad, so I will stick with this improved model.\n",
    "\n",
    "| **Base Model** | **Tuned Model** |\n",
    "|---|---|\n",
    "| <img src=\"report/figures/XGBoost.png\" alt=\"Drawing\" style=\"width: 350px;\"/>  | <img src=\"report/figures/tuned_model_CM.png\" alt=\"Drawing\" style=\"width: 350px;\"/> |\n",
    "\n",
    "### Feature Importance: Xgboost \n",
    "There are two things that need consideration when using xgBoost for understanding feature importance: the features that are doing the *most* work in splitting the data, and the automatically generated feature importance ranking that is done in xgBoost.\n",
    "\n",
    "#### Base Model Trees\n",
    "I plotted some estimators in the xgboost learners to see which features are doing the most work in splitting the data. I chose to focus on the **first** and **second** tree in the ensemble. On simple models, the first two trees may be enough to gain a *strong* understanding. This model has three levels and eight distinct types.\n",
    "\n",
    "| **First tree in the ensemble** | **Second tree in the ensemble** |\n",
    "|---|---|\n",
    "| <img src=\"report/figures/firstXGraph.png\" alt=\"Drawing\" style=\"width: 500px;\"/>  | <img src=\"report/figures/secondXGraph.png\" alt=\"Drawing\" style=\"width: 500px;\"/> |\n",
    "\n",
    "#### Tuned Model Trees\n",
    "The tuned model has a more complex tree that goes down six levels, for each of its estimators. This model segmented the data into 36 distinct types; you can see this by counting the number of leaf nodes.\n",
    "\n",
    "#### First tree in the esemble\n",
    "![tuned tree](report/figures/first_tuned_model_graph.png)\n",
    "\n",
    "\n",
    "#### Second tree in the esemble\n",
    "![tuned tree](report/figures/second_tuned_model_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5(a). Feature Importance: Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until cv error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration: 251\n"
     ]
    }
   ],
   "source": [
    "params_dict = model.get_params()\n",
    "xgdmat = xgb.DMatrix(X, y_train) # Create our DMatrix to make XGBoost more efficient\n",
    "testdmat = xgb.DMatrix(X_test)\n",
    "\n",
    "cv_xgb = xgb.cv(params = params_dict, dtrain = xgdmat, num_boost_round = 3000, nfold = folds,\n",
    "                metrics =['map'],\n",
    "                early_stopping_rounds = 100) # Look for early stopping that minimizes error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = xgb.train(params_dict, xgdmat, num_boost_round = 251)\n",
    "y_pred = bst.predict(testdmat,ntree_limit=bst.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "y_pred[y_pred > 0.5] = 1\n",
    "y_pred[y_pred <= 0.5] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "importances = bst.get_fscore()\n",
    "importance_frame = pd.DataFrame({'Importance': list(importances.values()), 'Feature': list(importances.keys())})\n",
    "importance_frame.sort_values(by = 'Importance', inplace = True)\n",
    "importance_frame.plot(kind = 'barh', x = 'Feature', figsize = (8,14));\n",
    "pyplt.title('XGBOOST TUNED FEATURE IMPORTANCE\\n')\n",
    "pyplt.savefig('report/figures/featureImportance_tuned.png', dpi=200, bbox_inches='tight')\n",
    "pyplt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5(b). Feature Importance: Information Gain based Ranking \n",
    "One can think on **information gain** as a measurement of *informativeness* of a feature with respect to the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tools.create_feature_map(list(importances.keys()),'data/xgb_model_feature_map.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "information_gain_list = tools.order_features_by_gains(bst, 'data/xgb_model_feature_map.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_importances = {}\n",
    "for i in range(len(information_gain_list)):\n",
    "    feat, info = information_gain_list[i]\n",
    "    info_importances[feat] = round(info['gain'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.5)\n",
    "importance_frame = pd.DataFrame({'Gain': list(info_importances.values()), 'Feature': list(info_importances.keys())})\n",
    "importance_frame.sort_values(by = 'Gain', inplace = True)\n",
    "importance_frame.plot(kind = 'barh', x = 'Feature', figsize = (8,14), color = 'green');\n",
    "pyplt.title('INFORMATION-GAIN BASED FEATURE IMPORTANCE\\n')\n",
    "pyplt.savefig('report/figures/featureImportance_informationGain.png', dpi=200, bbox_inches='tight')\n",
    "pyplt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5(c). Feature Importance: Cover based Ranking \n",
    "\n",
    "The cover is the sum of second order gradient in each node, and intuitively it represents the number of data points affected by the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_importances = {}\n",
    "for i in range(len(information_gain_list)):\n",
    "    feat, info = information_gain_list[i]\n",
    "    info_importances[feat] = round(info['cover'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.5)\n",
    "importances = bst.get_fscore()\n",
    "importance_frame = pd.DataFrame({'Cover': list(info_importances.values()), 'Feature': list(info_importances.keys())})\n",
    "importance_frame.sort_values(by = 'Cover', inplace = True)\n",
    "importance_frame.plot(kind = 'barh', x = 'Feature', figsize = (8,14), color = 'grey');\n",
    "pyplt.title('COVER BASED FEATURE IMPORTANCE\\n')\n",
    "pyplt.savefig('report/figures/featureImportance_cover.png', dpi=200, bbox_inches='tight')\n",
    "pyplt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "| **XGBoost Generated Feature Importance** | **Information Gain Based Feature Importance** | **Cover Based Feature Importance** |\n",
    "|---|---|---|\n",
    "| <img src=\"report/figures/featureImportance_tuned.png\" alt=\"Drawing\" style=\"width: 350px;\"/>  | <img src=\"report/figures/featureImportance_informationGain.png\" alt=\"Drawing\" style=\"width: 350px;\"/>  | <img src=\"report/figures/featureImportance_cover.png\" alt=\"Drawing\" style=\"width: 350px;\"/> |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
